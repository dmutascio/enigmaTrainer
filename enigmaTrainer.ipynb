{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4385c7-1962-465c-9beb-8b3961bad408",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow\n",
    "pip install requests\n",
    "pip install pandas\n",
    "pip install matplotlib\n",
    "pip install torch\n",
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceca19f-6fd5-41c8-8cb2-3cb30f93efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db74d41-3888-4e19-b847-0e484d116dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_json_objects(obj1, obj2):\n",
    "    merged_data = {}\n",
    "\n",
    "    #add ojb1 into holding dict. if key is found in both, merge them then add\n",
    "    for key, value in obj1.items():\n",
    "        if key in obj2:\n",
    "            merged_data[key] = value+obj2[key]\n",
    "        else:\n",
    "            merged_data[key] = value\n",
    "\n",
    "    #add obj2 into holding dict only if key not in obj1\n",
    "    for key, value in obj2.items():\n",
    "        if key not in merged_data:\n",
    "            merged_data[key] = value\n",
    "            \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2849723e-2887-40bb-9b36-fef695a15a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(url, next_page_token=None):\n",
    "    # Make a request to the stock price endpoint\n",
    "    apiKey = 'PKVVMO4VIR8019Z87DV6'\n",
    "    apiSecret = 'XlEHA9LpAe3lCmbASJY1Ffze3aRhmCdkhvM1SBnf'\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"APCA-API-KEY-ID\": apiKey,\n",
    "        \"APCA-API-SECRET-KEY\": apiSecret\n",
    "    }\n",
    "    response = requests.get(url+next_page_token, headers=headers)\n",
    "    \n",
    "    # Check if the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        json_data = response.json()\n",
    "        bars = json_data['bars']\n",
    "        # Extract stock data from the current page\n",
    "        current_page_data = {key: bars[key] for key in bars.keys()}\n",
    "        # Check if there is a next page\n",
    "        next_page_token = json_data.get('next_page_token')\n",
    "        if next_page_token is not None:\n",
    "            # Recursively fetch and merge the next page data\n",
    "            next_page_data = get_stock_data(url, '&page_token='+next_page_token)\n",
    "            current_page_data = merge_json_objects(current_page_data, next_page_data)\n",
    "\n",
    "        return current_page_data\n",
    "\n",
    "    else:\n",
    "        # Handle the case where the API request was not successful\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "182f3456-986a-4034-bd56-5666eed12213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "#gets stock data and cleans it\n",
    "def stock_data(symbols, pathParam, queryParams):\n",
    "    #base url to build off of\n",
    "    base_url = \"https://data.alpaca.markets/v2/stocks/\"\n",
    "\n",
    "    #add specific api path\n",
    "    base_url += pathParam\n",
    "\n",
    "    #add stock symbols to url\n",
    "    base_url += 'symbols='\n",
    "    for symbol in symbols:\n",
    "        base_url += symbol + \"%2C\";\n",
    "        base_url[:-3]\n",
    "\n",
    "    #add params to urls\n",
    "    for key, value in queryParams.items():\n",
    "        base_url += \"&\";\n",
    "        base_url += key;\n",
    "        base_url += \"=\";\n",
    "        base_url += value;\n",
    "\n",
    "    #get merged price data\n",
    "    results = get_stock_data(base_url, '')\n",
    "\n",
    "    #create list of symbols. FIXME: this could probably use the symbols param. dont know which is faster\n",
    "    symbols = list(results.keys())\n",
    "    #create empty dict to hold each symbols data\n",
    "    tables = {}\n",
    "    for symbol in symbols:\n",
    "        #get specific symbol's data\n",
    "        data = results[symbol]\n",
    "        \n",
    "        # Convert timestamp strings to datetime objects\n",
    "        for point in data:\n",
    "            #FIXME: make sure every endpoint includes 't' measure\n",
    "            point[\"t\"] = datetime.strptime(point[\"t\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        \n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "            \n",
    "            \n",
    "        # FIXME: This is not needed in prod. Wasted computation. Not agnostic\n",
    "        df.rename(columns={'c': 'close', 'h':'high', 'l':'low', 'n':'trade_count', 'o':'open', 't':'timestamp', 'v':'bar_volume', 'vw':'price_volume_weighted'}, inplace=True)\n",
    "    \n",
    "        # add df to dictionary of dfs\n",
    "        tables[symbol] = df\n",
    "\n",
    "    return tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd40522e-b469-4ff7-828c-3d1ad75fdc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_MACD(data, config):\n",
    "    firstPerdiod = config['firstPeriod']\n",
    "    secondPeriod = config['secondPeriod']\n",
    "\n",
    "    #Calculate short and long ewma used to calculate the final macd\n",
    "    macd1=pd.Series(data['price_volume_weighted'].ewm(span=firstPerdiod, min_periods=1, adjust=False).mean())\n",
    "    macd2=pd.Series(data['price_volume_weighted'].ewm(span=secondPeriod, min_periods=1, adjust=False).mean())\n",
    "    macd = pd.Series(macd1 - macd2, name='value')\n",
    "    indicator = pd.concat([data, macd], axis=1)\n",
    "    return indicator\n",
    "    \n",
    "\n",
    "def create_RSI(data, config):\n",
    "    period = config['period']\n",
    "    price_change=pd.Series(data['price_volume_weighted'].diff())\n",
    "\n",
    "    # Calculate the average gain and average loss over a 14-day period\n",
    "    gain=pd.Series(price_change.apply(lambda x: x if x > 0 else 0))\n",
    "    loss=pd.Series(price_change.apply(lambda x: abs(x) if x < 0 else 0))\n",
    "    avg_gain=pd.Series(gain.rolling(window=period, min_periods=1).mean())\n",
    "    avg_loss=pd.Series(loss.rolling(window=period, min_periods=1).mean())\n",
    "        \n",
    "    # Calculate the relative strength (RS)\n",
    "    rs = avg_gain / avg_loss\n",
    "\n",
    "    # Calculate the RSI\n",
    "    rsi = pd.Series(100 - (100 / (1 + rs)), name='value')\n",
    "    indicator = pd.concat([data, rsi], axis=1)\n",
    "    return indicator\n",
    "    \n",
    "    \n",
    "def create_MA(data, config):\n",
    "    period = config['period']\n",
    "\n",
    "    #Calculate simple moving avg\n",
    "    ma=pd.Series(data['price_volume_weighted'].rolling(window=period).mean(), name='value')\n",
    "    indicator=pd.concat([data, ma], axis=1)\n",
    "    return indicator\n",
    "    \n",
    "\n",
    "def create_PriceDelta(data, config):\n",
    "    period = config['period']\n",
    "\n",
    "    #Calculate simple price change from previous\n",
    "    delta=pd.Series(data['price_volume_weighted'] - data['price_volume_weighted'].shift(period), name='value')\n",
    "    indicator=pd.concat([data, delta], axis=1)\n",
    "    return indicator\n",
    "\n",
    "\n",
    "def create_PriceDeltaPercent(data, config):\n",
    "    period = config['period']\n",
    "    point = config['point']\n",
    "\n",
    "    #Calculate simple price change from previous (percent)\n",
    "    delta=pd.Series(abs(data['price_volume_weighted']-data['price_volume_weighted'].shift(period)))\n",
    "    final = pd.Series((delta/data['price_volume_weighted'].shift(period))*100, name='value') if point == 'initial' else pd.Series((delta/data['price_volume_weighted'])*100, name='value')\n",
    "    indicator=pd.concat([data, final], axis=1)\n",
    "    return indicator\n",
    "\n",
    "def create_EWMA(data, config):\n",
    "    period = config['period']\n",
    "\n",
    "    #Calculate ewma\n",
    "    ewma = pd.Series(data['price_volume_weighted'].ewm(span=period, min_periods=1).mean(), name='value')\n",
    "    indicator=pd.concat([data, ewma], axis=1)\n",
    "    return indicator\n",
    "\n",
    "#map indicator names to their build functions\n",
    "indicator_functions = {\n",
    "    'MACD': create_MACD,\n",
    "    'RSI': create_RSI,\n",
    "    'MA': create_MA,\n",
    "    'PriceDelta': create_PriceDelta,\n",
    "    'PriceDeltaPercent': create_PriceDeltaPercent,\n",
    "    'EWMA': create_EWMA\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dc08627-70e7-4249-ab90-32dc75c7d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indicators(stocks, indicator_config):\n",
    "    indicators_master = {}\n",
    "\n",
    "    #for each symbol calculate all the indicators given in the config param\n",
    "    for symbol, data in stocks.items():\n",
    "        symbol_indicators = {}\n",
    "        for indicator_name, config in indicator_config.items():\n",
    "            \n",
    "            # Use the indicator_functions dictionary to get the appropriate function\n",
    "            indicator_function = indicator_functions.get(indicator_name)\n",
    "            if indicator_function:\n",
    "                \n",
    "                # Call the selected function with the corresponding configuration\n",
    "                indicator_value = indicator_function(data, config)\n",
    "                symbol_indicators[indicator_name] = indicator_value\n",
    "            else:\n",
    "                print(f\"Warning: Unknown indicator '{indicator}'. Skipping.\")\n",
    "        indicators_master[symbol] = symbol_indicators\n",
    "    return indicators_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09e87389-cfe4-4016-a4a8-18f564705fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_signals(stocks, signal_memory, cutin, cutoff):\n",
    "    signals_final = {}\n",
    "\n",
    "    #Create the signals for each symbol\n",
    "    for symbol, bars in stocks.items():\n",
    "\n",
    "        #flip list for easier ewma calculation for future prices, with higher weight for more recent prices\n",
    "        flipped = bars['price_volume_weighted'][::-1]\n",
    "        flipped_ewma = flipped.ewm(span=signal_memory, min_periods=0, adjust=False).mean()\n",
    "\n",
    "        #flip list back\n",
    "        ewma = flipped_ewma[::-1]\n",
    "\n",
    "        #signal is simply the change from now to ewma of future x prices\n",
    "        future_change = ewma - bars['price_volume_weighted']\n",
    "        signal = pd.Series(future_change/bars['price_volume_weighted'], name = 'value')\n",
    "        full_df = pd.concat([bars[['timestamp']], signal], axis=1)\n",
    "        signals_final[symbol] = full_df.iloc[cutin:-cutoff].reset_index(drop=True)\n",
    "    return signals_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6145c2d-4e47-4c59-8859-75bef6d6d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(data):\n",
    "    \n",
    "    # simple min max scaling to normalize indicators\n",
    "    min_val = np.min(data['value'])\n",
    "    max_val = np.max(data['value'])\n",
    "    scaled_data = (data['value'] - min_val) / (max_val - min_val)\n",
    "    scaled=pd.concat([data['timestamp'], scaled_data], axis=1)\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4023531e-4ad4-4f5a-9cff-f26b2493a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and format data\n",
    "def create_features(indicators, cutin, cutoff):\n",
    "    features = {}\n",
    "    features_scaled={}\n",
    "    outputs = {}\n",
    "\n",
    "    #normalize every indicator for each symbol (feature=indicator)\n",
    "    for symbol, indicator_dict in indicators.items():\n",
    "        feature = {}\n",
    "        feature_scaled = {}\n",
    "        output = []\n",
    "        for indicator_name, indicator_value in indicator_dict.items():\n",
    "            feature[indicator_name] = indicator_value[['value', 'timestamp']].iloc[cutin:-cutoff].reset_index(drop=True)\n",
    "            feature_scaled[indicator_name] = min_max_scaling(feature[indicator_name])\n",
    "        features[symbol]=feature\n",
    "        features_scaled[symbol]=feature_scaled\n",
    "    return features, features_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aec2def4-af0d-4ad9-9947-24d3df16e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main flow\n",
    "def main(symbols, queryParams, pathParam, indicator_config, signal_memory):\n",
    "\n",
    "    #define values for trimming features. needed because some features are caclulated from periods\n",
    "    #Ex: moving avg 50 pts needs 50 data points (if not using today) to calculate 51st value, so first 50 values are NaN\n",
    "    #FIXME: this should be automatic\n",
    "    cutin = 51\n",
    "    cutoff = 1\n",
    "\n",
    "    #get stock data for given symbols\n",
    "    stock_bars = stock_data(symbols, pathParam, queryParams)\n",
    "\n",
    "    #calculate indicators from stock data\n",
    "    indicators = create_indicators(stock_bars, indicator_config)\n",
    "\n",
    "    #create features from indicators (clean and normalize)\n",
    "    features, features_scaled = create_features(indicators, cutin, cutoff)\n",
    "\n",
    "    #create signals from stock data\n",
    "    signals = create_signals(stock_bars, signal_memory, cutin, cutoff)\n",
    "    data = {}\n",
    "    for symbol in symbols:\n",
    "        data[symbol] = {\n",
    "            'bars': stock_bars[symbol].iloc[cutin:-cutoff].reset_index(drop=True),\n",
    "            'indicators': indicators[symbol],\n",
    "            'features': features[symbol],\n",
    "            'features_scaled': features_scaled[symbol],\n",
    "            'signals': signals[symbol]\n",
    "            \n",
    "        }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ad9783a-f44e-412b-b759-985aeaf7dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init config and get stock price data\n",
    "symbols = ['AAPL', 'MSFT', 'BRK.B', 'AGL', 'NVDA']\n",
    "queryParams = {\n",
    "    'timeframe':'3Min',\n",
    "    'adjustment': 'all',\n",
    "    'feed': 'sip',\n",
    "    'sort': 'asc',\n",
    "    'start': '2018-09-01T00%3A00%3A00Z',\n",
    "    'limit': '10000'\n",
    "             }\n",
    "pathParam = 'bars?'\n",
    "\n",
    "#Notice: periods are in # measurements, not days\n",
    "indicator_config = {\n",
    "    'MACD': {\n",
    "        'firstPeriod': 12,\n",
    "        'secondPeriod': 26,\n",
    "    },\n",
    "    'RSI': {\n",
    "        'period': 14\n",
    "    },\n",
    "    'MA': {\n",
    "        'period': 50\n",
    "    },\n",
    "    'EWMA': {\n",
    "        'period': 20\n",
    "    },\n",
    "    'PriceDelta': {\n",
    "        'period': 1\n",
    "    },\n",
    "    'PriceDeltaPercent': {\n",
    "        'period': 1,\n",
    "        'point': 'initial'\n",
    "    }\n",
    "}\n",
    "\n",
    "data = main(symbols, queryParams, pathParam, indicator_config, signal_memory=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ff797e4-b384-4c13-9c58-5725a28735ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Build net\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Build net\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "#define neural net class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "def create_model(data, lr, epoch_length, shuffle_bool):\n",
    "    training_loss={}\n",
    "    input_size = len(data['features_scaled'])\n",
    "    train_size = int(0.8 * len(data['features_scaled'][list(data['features_scaled'].keys())[0]]))\n",
    "\n",
    "    #unpack each feature (map) into a list; zip features at each step together\n",
    "    feature_scaled = list(zip(*[feature['value'] for feature in data['features_scaled'].values()]))\n",
    "\n",
    "    output = np.array(data['signals']['value'])\n",
    "\n",
    "    #split up feature dataset into train and test\n",
    "    feature_train = feature_scaled[:train_size]\n",
    "    output_train = output[:train_size]\n",
    "    feature_test = feature_scaled[train_size:]\n",
    "    output_test = output[train_size:]\n",
    "    \n",
    "    X_train_tensor=torch.FloatTensor(feature_train)\n",
    "    y_train_tensor = torch.FloatTensor(output_train)\n",
    "    \n",
    "    X_test_tensor = torch.FloatTensor(feature_test)\n",
    "    y_test_tensor = torch.FloatTensor(output_test)\n",
    "    \n",
    "    # Set hyperparameters\n",
    "    hidden_size = 64\n",
    "    output_size = 1\n",
    "    learning_rate = lr\n",
    "    epochs = epoch_length\n",
    "    \n",
    "    # Create an instance of the model\n",
    "    model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "        \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "    # Prepare DataLoader for training\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=shuffle_bool)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # Forward pass\n",
    "            model_output = model(inputs)\n",
    "                \n",
    "            # Compute the loss\n",
    "            labels = labels.view_as(model_output)\n",
    "            loss = criterion(model_output, labels)\n",
    "    \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "        \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Print the training loss for each epoch\n",
    "        training_loss[epoch]=loss.item()\n",
    "\n",
    "    return model, training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae1607-5903-4dc9-8fd1-b518ab3752d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac37eaa-9b4a-4aa0-a1eb-0862d4b3414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__ (self, model, training_loss, symbol, lr, epoch_length, shuffle_bool):\n",
    "        self.model = model\n",
    "        self.training_loss = training_loss\n",
    "        self.symbol = symbol\n",
    "        self.lr = lr\n",
    "        self.epoch_length = epoch_length\n",
    "        self.shuffle_bool = shuffle_bool\n",
    "\n",
    "    def graph_loss(self):\n",
    "        plt.plot(self.training_loss.keys(), self.training_loss.values(), marker='o', linestyle='-', color='b')\n",
    "        plt.title(f'Training Loss Over Time with lr:{self.lr}, epoch:{self.epoch_length}, shuffle:{self.shuffle_bool}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def graph_output(self, data, smooth_window=1, scale_center=0, scale_size=10, start=False, granularity=False):\n",
    "        feature_scaled = list(zip(*[feature['value'] for feature in data['features_scaled'].values()]))\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "    \n",
    "            #Create price graph scatter graph\n",
    "            test_outputs = self.model(torch.FloatTensor(feature_scaled))\n",
    "            test_outputs_np = test_outputs.numpy().squeeze()\n",
    "            x = np.arange(len(test_outputs_np))\n",
    "            if not start:\n",
    "                start = 0\n",
    "            if not granularity:\n",
    "                granularity = len(x)-start\n",
    "\n",
    "            start = int(len(x)*start)\n",
    "            granularity = int(len(x)*granularity)\n",
    "\n",
    "            if start+granularity >= len(x):\n",
    "                granularity = len(x)-start\n",
    "            \n",
    "            plt.figure(figsize=(15, 6))\n",
    "            plt.scatter(data['signals']['timestamp'].iloc[start:start+granularity], data['bars']['price_volume_weighted'].iloc[start:start+granularity], c='b', marker='o', s=5, label='Markers', edgecolors='none') \n",
    "    \n",
    "            #create model output graph\n",
    "            kernel = np.ones(smooth_window) / smooth_window\n",
    "            smoothed_array = np.convolve(test_outputs_np, kernel, mode='same')\n",
    "            scale_limit = min(data['bars']['price_volume_weighted'])\n",
    "            scaled_array = np.interp(smoothed_array, (min(smoothed_array), max(smoothed_array)), (scale_center-scale_size, scale_center+scale_size))\n",
    "            colors = ['red' if val < 0 else 'green' for val in smoothed_array[start:start+granularity]]\n",
    "            plt.scatter(data['signals']['timestamp'].iloc[start:start+granularity], scaled_array[start:start+granularity], c=colors, marker='o', s=5, label='Markers', edgecolors='none') \n",
    "            plt.grid(True, linestyle='--', linewidth=0.5, color='gray', alpha=1)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8d0fe3-8c96-47c6-8924-2d5b90d9c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, training_loss = create_model(data=data['AGL'], lr=.00000001, epoch_length=2000, shuffle_bool=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23035250-5552-4907-83a3-255fbab58a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, training_loss = create_model(data=data['AAPL'], lr=.0000005, epoch_length=100, shuffle_bool=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f17db62-75f7-41fb-8fc7-6f1eb846805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, training_loss = create_model(data=data['AGL'], lr=.0000005, epoch_length=50, shuffle_bool=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f33bd-10aa-4ae5-b24d-00f5a06870ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    feature_scaled = list(zip(*[feature['value'] for feature in data['AGL']['features_scaled'].values()]))\n",
    "    model_outputs = model(torch.FloatTensor(feature_scaled))\n",
    "    model_outputs_np = model_outputs.numpy().squeeze()\n",
    "    scaled_array = np.interp(model_outputs_np, (min(model_outputs_np), max(model_outputs_np)), (1, 100))\n",
    "    print(len(model_outputs_np))\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(scaled_array)\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(model_outputs_np)\n",
    "    plt.show()\n",
    "\n",
    "    kernel = np.ones(1000) / 1000\n",
    "    smoothed_array = np.convolve(model_outputs_np, kernel, mode='valid')\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(smoothed_array)\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5, color='gray', alpha=1)\n",
    "    plt.show()\n",
    "\n",
    "    print(len(smoothed_array))\n",
    "    print(len(model_outputs_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60967cb-b91b-400c-8428-850405ff761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        zoom = 120\n",
    "        symbol = 'AGL'\n",
    "        model.eval()\n",
    "        feature_scaled = list(zip(*[feature['value'] for feature in data[symbol]['features_scaled'].values()]))\n",
    "        model_outputs = model(torch.FloatTensor(feature_scaled))\n",
    "        model_outputs_np = model_outputs.numpy().squeeze()\n",
    "        \n",
    "        #Create price graph scatter graph\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.scatter(data[symbol]['signals']['timestamp'].iloc[-zoom:], data[symbol]['bars']['price_volume_weighted'].iloc[-zoom:], c='b', marker='o', s=5, label='Markers', edgecolors='none') \n",
    "\n",
    "        #create model output graph\n",
    "        smooth_window = 10\n",
    "        kernel = np.ones(smooth_window) / smooth_window\n",
    "        smoothed_array = np.convolve(model_outputs_np, kernel, mode='same')\n",
    "        scale_limit = min(data[symbol]['bars']['price_volume_weighted'])\n",
    "        scaled_array = np.interp(smoothed_array, (min(smoothed_array), max(smoothed_array)), (6.9, 7.1))\n",
    "        colors = ['red' if val < 0 else 'green' for val in smoothed_array[-zoom:]]\n",
    "        plt.scatter(data[symbol]['signals']['timestamp'].iloc[-zoom:], scaled_array[-zoom:], c=colors, marker='o', s=5, label='Markers', edgecolors='none') \n",
    "\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5, color='gray', alpha=1)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5e2a0-ab62-4189-8551-fa8ad517bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### from collections import deque\n",
    "def test_model(model, data, bank):\n",
    "    pricing_data = data['bars']['price_volume_weighted'].values\n",
    "    bank = bank\n",
    "    with torch.no_grad():\n",
    "            model.eval()\n",
    "            feature_scaled = list(zip(*[feature['value'] for feature in data['features_scaled'].values()]))\n",
    "            model_outputs = model(torch.FloatTensor(feature_scaled))\n",
    "            model_outputs_np = model_outputs.numpy().squeeze()\n",
    "            data['signals']['output'] = model_outputs_np\n",
    "    \n",
    "            for choice in data['signals']['output']:\n",
    "            #    if choice > 0:\n",
    "                    #Buy\n",
    "            #    if choice < 0:\n",
    "                    #short\n",
    "                print(choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe1175-11c4-4293-bef6-da57692bdd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph signals\n",
    "zoom = 400000\n",
    "colors = ['red' if val <= 0 else 'green' for val in data['AAPL']['signals']['value'].iloc[-zoom:]]\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.scatter(data['AAPL']['signals']['timestamp'].iloc[-zoom:], data['AAPL']['bars']['price_volume_weighted'].iloc[-zoom:], c=colors, marker='o', s=5, label='Markers', edgecolors='none') \n",
    "plt.title(\"Your Plot Title\")\n",
    "plt.xlabel(\"X-axis Label\")\n",
    "plt.ylabel(\"Y-axis Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c46cff-b174-4c27-bd5f-00911296a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph feature\n",
    "data['AAPL']['features']['RSI'].plot(y='value', x='timestamp', kind='scatter', color='b', figsize=(20, 6))\n",
    "#data['AAPL']['bars'].plot(x='timestamp', y='price_volume_weighted', kind='line', linestyle='-', color='b', figsize=(20, 6))\n",
    "plt.title(f'Time vs feature')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('feature')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8675f085-d0ec-41f4-9f61-2148ca8df109",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['AAPL_Short_v0.1'] = Model(model, training_loss, 'AAPL', .0000005, 100, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74808db0-3ab6-4ac7-ad18-d8a443894107",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['AAPL_Short_v0.1'].graph_output(data['AAPL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e379d-d04f-4469-b094-5006edb4aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['AAPL_Short_v0.1'].graph_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd0632-eea5-492a-ae5f-55c377dbbcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_save(symbol, data, lr, epoch_length, shuffle_bool):\n",
    "    model, training_loss = create_model(data, lr, epoch_length, shuffle_bool)\n",
    "    models[symbol + \":\" + str(lr) + \":\" + str(epoch_length) + \":\" + str(shuffle_bool)] = Model(model, training_loss, symbol, lr, epoch_length, shuffle_bool)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd820ea3-09c7-40b3-af85-d8a572aa49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_save('AAPL', data['AAPL'], .001, 30, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7183c4b-b1e9-4667-847b-85c6f6929cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "for symbol, symbol_data in data.items():\n",
    "    for i in range(10, 1000, 20):\n",
    "        for j in range (2, 10):\n",
    "            lr =  .0001/2**j\n",
    "            epoch_length = i\n",
    "            print(\"creating model: \" + symbol + \":\" + str(lr) + \":\" + str(epoch_length) + \":\" + \"True\")\n",
    "            create_save(symbol, symbol_data, lr, epoch_length, True)\n",
    "            print(\"created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b698fd-36ca-4042-b03e-4756a65b1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8953ec-ec26-4997-9d88-2970a6b068be",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_output(models['AAPL:3.90625e-07:190:True'].model, data['AAPL'], scale_center=64, scale_size=50, start=.20, granularity=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39c64a-c1e9-4211-8393-956f5484b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_output(model, data, smooth_window=1, scale_center=0, scale_size=10, start=False, granularity=False):\n",
    "        feature_scaled = list(zip(*[feature['value'] for feature in data['features_scaled'].values()]))\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "    \n",
    "            #Create price graph scatter graph\n",
    "            test_outputs = model(torch.FloatTensor(feature_scaled))\n",
    "            test_outputs_np = test_outputs.numpy().squeeze()\n",
    "            x = np.arange(len(test_outputs_np))\n",
    "            if not start:\n",
    "                start = 0\n",
    "            if not granularity:\n",
    "                granularity = len(x)-start\n",
    "\n",
    "            start = int(len(x)*start)\n",
    "            granularity = int(len(x)*granularity)\n",
    "\n",
    "            if start+granularity >= len(x):\n",
    "                granularity = len(x)-start\n",
    "            \n",
    "            plt.figure(figsize=(15, 6))\n",
    "            plt.scatter(data['signals']['timestamp'].iloc[start:start+granularity], data['bars']['price_volume_weighted'].iloc[start:start+granularity], c='b', marker='o', s=5, label='Markers', edgecolors='none') \n",
    "    \n",
    "            #create model output graph\n",
    "            kernel = np.ones(smooth_window) / smooth_window\n",
    "            smoothed_array = np.convolve(test_outputs_np, kernel, mode='same')\n",
    "            scale_limit = min(data['bars']['price_volume_weighted'])\n",
    "            scaled_array = np.interp(smoothed_array, (min(smoothed_array), max(smoothed_array)), (scale_center-scale_size, scale_center+scale_size))\n",
    "            colors = ['red' if val < 0 else 'green' for val in smoothed_array[start:start+granularity]]\n",
    "            plt.scatter(data['signals']['timestamp'].iloc[start:start+granularity], scaled_array[start:start+granularity], c=colors, marker='o', s=5, label='Markers', edgecolors='none') \n",
    "            plt.grid(True, linestyle='--', linewidth=0.5, color='gray', alpha=1)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d9b94-2d39-410b-8c90-dca31bffd931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class trade:\n",
    "    def __init__(self, price, direction, time):\n",
    "        self.price = price\n",
    "        self.direction = direction\n",
    "        self.time = time\n",
    "    def get_return(self, current_price):\n",
    "        if self.direction == 'short':\n",
    "            #return self.price-current_price\n",
    "            return 0\n",
    "        else:\n",
    "            return current_price-self.price\n",
    "\n",
    "def simulate(model, data, bank=0, hold=3):\n",
    "    pricing_data = data['bars']['price_volume_weighted'].values\n",
    "    bank = bank\n",
    "    trades = deque([None] * hold)\n",
    "    with torch.no_grad():\n",
    "            model.eval()\n",
    "            feature_scaled = list(zip(*[feature['value'] for feature in data['features_scaled'].values()]))\n",
    "            model_outputs = model(torch.FloatTensor(feature_scaled))\n",
    "            signals = model_outputs.numpy().squeeze()\n",
    "\n",
    "            for time, signal in enumerate(signals):\n",
    "                #print(trades)\n",
    "                #create new trade\n",
    "                price = pricing_data[time]\n",
    "                if signal>0:\n",
    "                    direction = 'long'\n",
    "                else:\n",
    "                    direction = 'short'\n",
    "                new_trade = trade(price, direction, time)\n",
    "                trades.append(new_trade)\n",
    "                #bank -= price\n",
    "\n",
    "                #evaluate/sell old trade\n",
    "                old_trade = trades.popleft()\n",
    "                if old_trade is not None:\n",
    "                    returns = old_trade.get_return(price)\n",
    "                    bank += returns\n",
    "    return bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4913c8-17a3-4e6c-bd86-17a13e47c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = simulate(model=models['AAPL:3.90625e-07:190:True'].model, data=data['AAPL'], hold=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722c3d2-292e-42c8-8777-a65a339fe644",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2978437-4c18-470a-817d-6a9712236659",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i in range(1, 1000, 50):\n",
    "    print(i)\n",
    "    test.append(simulate(model=models['AAPL:2.5e-05:10:True'].model, data=data['NVDA'], hold=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f075d-639a-43e0-adc3-f04bf1582e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the array\n",
    "plt.plot(test)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d957f7-bde7-4869-9269-97f73e5408a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
